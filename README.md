# Improving Human Verification of LLM Reasoning through Interactive Explanation Interfaces

This repository summarizes the paper **Improving Human Verification of LLM Reasoning through Interactive Explanation Interfaces**.

## Overview
Large Language Models often generate long chain-of-thought (CoT) explanations that overwhelm users.  
This project introduces **interactive explanation formats**—iCoT, iPoT, and iGraph—to reduce cognitive load and improve reasoning verification.

### Figure 1 – Static CoT vs Interactive Interface
![Figure 1a](figures/figure1a.png)
![Figure 1b](figures/figure1b.png)

### Figure 2 – System Pipeline
![Figure 2](figures/pipeline.png)

### Figure 3 – Verification Accuracy
![Figure 3](figures/accuracy.png)

### Figure 4 – Error Localization Accuracy
![Figure 4](figures/localization.png)

### Figure 5 – Response Time
![Figure 5](figures/time.png)

### Figure 6 – User Survey Results
![Figure 6](figures/survey.png)

### Figure 8 – Interactive CoT Interface
![Figure 8](figures/icot.png)

### Figure 9 – Interactive Program-of-Thought
![Figure 9](figures/ipot.png)

### Figure 10 – Interactive Graph
![Figure 10](figures/igraph.png)


## Key Takeaways
- Interactive reasoning significantly improves users’ ability to verify LLM reasoning.
- Graph-based explanations outperform standard CoT.
- Structured, interactive interfaces reduce cognitive load.
- The approach generalizes to many reasoning domains.

---

## Future Work
- Adaptive interactive reasoning
- Automatic error tagging
- Expanding to scientific & legal domains
